{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves as an introduction to the new functionality added to the nuScenes devkit for the prediction challenge.\n",
    "\n",
    "It is organized into the following sections:\n",
    "\n",
    "1. Data splis for the challenge\n",
    "2. Getting past and future data for an agent \n",
    "3. Changes to the Map API\n",
    "4. Overview of Input Representation\n",
    "5. Baseline Model Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes import NuScenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-mini...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "911 instance,\n",
      "12 sensor,\n",
      "120 calibrated_sensor,\n",
      "31206 ego_pose,\n",
      "8 log,\n",
      "10 scene,\n",
      "404 sample,\n",
      "31206 sample_data,\n",
      "18538 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 2.5 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.1 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "nuscenes = NuScenes('v1.0-mini', dataroot='/home/freddyboulton/prediction_nas/sets/nuscenes/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Splits for the Prediction Challenge\n",
    "\n",
    "This section assumes basic familiarity with the nuScenes [schema](https://www.nuscenes.org/data-format?externalData=all&mapData=all&modalities=Any).\n",
    "\n",
    "The goal of the NuScenes prediction challenge is to predict the future location of agents in the nuScenes dataset. Agents are indexed by an instance token and a sample annotation. To get a list of agents in the train and val split of the challenge, we provide a function called `get_prediction_challenge_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.eval.predict.splits import get_prediction_challenge_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_train = get_prediction_challenge_split(\"mini_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_prediction_challenge_split` function returns a list of strings of the form `{instance_token}_{sample_token}`. In the next section, we show how to use an instance token and sample token to query data for the prediction challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['48d58b69b40149aeb2e64aa4b1a9192f_ca9a282c9e77460f8360f564131a8af5',\n",
       " '48d58b69b40149aeb2e64aa4b1a9192f_39586f9d59004284a7114a68825e8eec',\n",
       " '48d58b69b40149aeb2e64aa4b1a9192f_356d81f38dd9473ba590f39e266f54e5',\n",
       " '48d58b69b40149aeb2e64aa4b1a9192f_e0845f5322254dafadbbed75aaa07969',\n",
       " '48d58b69b40149aeb2e64aa4b1a9192f_c923fe08b2ff4e27975d2bf30934383b']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting past and future data for an agent\n",
    "\n",
    "We provide a class called `PredictHelper` that provides methods for querying past and future data for an agent. This class is instantited by wrapping an instance of the `nuScenes` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.predict import PredictHelper\n",
    "helper = PredictHelper(nuscenes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the data for an agent at a particular point in time, use the `get_sample_annotation` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_token, sample_token = mini_train[0].split(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = helper.get_sample_annotation(instance_token, sample_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token': '6b89da9bf1f84fd6a5fbe1c3b236f809',\n",
       " 'sample_token': 'ca9a282c9e77460f8360f564131a8af5',\n",
       " 'instance_token': '48d58b69b40149aeb2e64aa4b1a9192f',\n",
       " 'visibility_token': '2',\n",
       " 'attribute_tokens': ['ab83627ff28b465b85c427162dec722f'],\n",
       " 'translation': [378.888, 1153.348, 0.865],\n",
       " 'size': [0.775, 0.769, 1.711],\n",
       " 'rotation': [-0.5527590208259255, 0.0, 0.0, 0.8333411455673865],\n",
       " 'prev': '',\n",
       " 'next': '216bbbd8e01c450a8fabe9d47433c10a',\n",
       " 'num_lidar_pts': 2,\n",
       " 'num_radar_pts': 0,\n",
       " 'category_name': 'human.pedestrian.adult'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the future/past of an agent, use the `get_past_for_agent`/`get_future_for_agent` methods. If the `in_agent_frame` parameter is set to true, the coordinates will be in the agents local coordinate frame. Otherwise, they will be in the global frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01285847,  0.62956228],\n",
       "       [-0.02610585,  1.26004583],\n",
       "       [-0.03804305,  1.88999702],\n",
       "       [ 0.00629423,  2.46120994],\n",
       "       [ 0.05024259,  3.03334414],\n",
       "       [ 0.09457987,  3.60455707]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_xy_local = helper.get_future_for_agent(instance_token, sample_token, seconds=3, in_agent_frame=True)\n",
    "future_xy_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 378.655, 1152.763],\n",
       "       [ 378.422, 1152.177],\n",
       "       [ 378.188, 1151.592],\n",
       "       [ 377.925, 1151.083],\n",
       "       [ 377.662, 1150.573],\n",
       "       [ 377.399, 1150.064]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_xy_global = helper.get_future_for_agent(instance_token, sample_token, seconds=3, in_agent_frame=False)\n",
    "future_xy_global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can also return the entire annotation record by passing `just_xy=False`. However in this case, `in_agent_frame` is not taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'token': '216bbbd8e01c450a8fabe9d47433c10a',\n",
       "  'sample_token': '39586f9d59004284a7114a68825e8eec',\n",
       "  'instance_token': '48d58b69b40149aeb2e64aa4b1a9192f',\n",
       "  'visibility_token': '2',\n",
       "  'attribute_tokens': ['ab83627ff28b465b85c427162dec722f'],\n",
       "  'translation': [378.655, 1152.763, 0.873],\n",
       "  'size': [0.775, 0.769, 1.711],\n",
       "  'rotation': [-0.5527590208259255, 0.0, 0.0, 0.8333411455673865],\n",
       "  'prev': '6b89da9bf1f84fd6a5fbe1c3b236f809',\n",
       "  'next': 'ea18f90581fc46978c4954d6f0147fb0',\n",
       "  'num_lidar_pts': 2,\n",
       "  'num_radar_pts': 0,\n",
       "  'category_name': 'human.pedestrian.adult'},\n",
       " {'token': 'ea18f90581fc46978c4954d6f0147fb0',\n",
       "  'sample_token': '356d81f38dd9473ba590f39e266f54e5',\n",
       "  'instance_token': '48d58b69b40149aeb2e64aa4b1a9192f',\n",
       "  'visibility_token': '2',\n",
       "  'attribute_tokens': ['ab83627ff28b465b85c427162dec722f'],\n",
       "  'translation': [378.422, 1152.177, 0.882],\n",
       "  'size': [0.775, 0.769, 1.711],\n",
       "  'rotation': [-0.5527590208259255, 0.0, 0.0, 0.8333411455673865],\n",
       "  'prev': '216bbbd8e01c450a8fabe9d47433c10a',\n",
       "  'next': '80d5ba393dff41a49235f58fab2d383f',\n",
       "  'num_lidar_pts': 1,\n",
       "  'num_radar_pts': 0,\n",
       "  'category_name': 'human.pedestrian.adult'},\n",
       " {'token': '80d5ba393dff41a49235f58fab2d383f',\n",
       "  'sample_token': 'e0845f5322254dafadbbed75aaa07969',\n",
       "  'instance_token': '48d58b69b40149aeb2e64aa4b1a9192f',\n",
       "  'visibility_token': '2',\n",
       "  'attribute_tokens': ['ab83627ff28b465b85c427162dec722f'],\n",
       "  'translation': [378.188, 1151.592, 0.865],\n",
       "  'size': [0.775, 0.769, 1.711],\n",
       "  'rotation': [-0.5527590208259255, 0.0, 0.0, 0.8333411455673865],\n",
       "  'prev': 'ea18f90581fc46978c4954d6f0147fb0',\n",
       "  'next': '15a1f0f5abbd4ecf9d73ca6a03d07cb9',\n",
       "  'num_lidar_pts': 3,\n",
       "  'num_radar_pts': 0,\n",
       "  'category_name': 'human.pedestrian.adult'},\n",
       " {'token': '15a1f0f5abbd4ecf9d73ca6a03d07cb9',\n",
       "  'sample_token': 'c923fe08b2ff4e27975d2bf30934383b',\n",
       "  'instance_token': '48d58b69b40149aeb2e64aa4b1a9192f',\n",
       "  'visibility_token': '3',\n",
       "  'attribute_tokens': ['ab83627ff28b465b85c427162dec722f'],\n",
       "  'translation': [377.925, 1151.083, 0.882],\n",
       "  'size': [0.775, 0.769, 1.711],\n",
       "  'rotation': [-0.5527590208259255, 0.0, 0.0, 0.8333411455673865],\n",
       "  'prev': '80d5ba393dff41a49235f58fab2d383f',\n",
       "  'next': 'c6d92f8785e443309d79fda6dbf5ac12',\n",
       "  'num_lidar_pts': 2,\n",
       "  'num_radar_pts': 0,\n",
       "  'category_name': 'human.pedestrian.adult'},\n",
       " {'token': 'c6d92f8785e443309d79fda6dbf5ac12',\n",
       "  'sample_token': 'f1e3d9d08f044c439ce86a2d6fcca57b',\n",
       "  'instance_token': '48d58b69b40149aeb2e64aa4b1a9192f',\n",
       "  'visibility_token': '2',\n",
       "  'attribute_tokens': ['ab83627ff28b465b85c427162dec722f'],\n",
       "  'translation': [377.662, 1150.573, 0.898],\n",
       "  'size': [0.775, 0.769, 1.711],\n",
       "  'rotation': [-0.5527590208259255, 0.0, 0.0, 0.8333411455673865],\n",
       "  'prev': '15a1f0f5abbd4ecf9d73ca6a03d07cb9',\n",
       "  'next': 'f1f57ad4e8824cbb8fcf77fb737e7b76',\n",
       "  'num_lidar_pts': 4,\n",
       "  'num_radar_pts': 0,\n",
       "  'category_name': 'human.pedestrian.adult'},\n",
       " {'token': 'f1f57ad4e8824cbb8fcf77fb737e7b76',\n",
       "  'sample_token': '4f545737bf3347fbbc9af60b0be9a963',\n",
       "  'instance_token': '48d58b69b40149aeb2e64aa4b1a9192f',\n",
       "  'visibility_token': '3',\n",
       "  'attribute_tokens': ['ab83627ff28b465b85c427162dec722f'],\n",
       "  'translation': [377.399, 1150.064, 0.915],\n",
       "  'size': [0.775, 0.769, 1.711],\n",
       "  'rotation': [-0.5527590208259255, 0.0, 0.0, 0.8333411455673865],\n",
       "  'prev': 'c6d92f8785e443309d79fda6dbf5ac12',\n",
       "  'next': '637915e12be44a49848d4e7860455725',\n",
       "  'num_lidar_pts': 3,\n",
       "  'num_radar_pts': 0,\n",
       "  'category_name': 'human.pedestrian.adult'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helper.get_future_for_agent(instance_token, sample_token, seconds=3, in_agent_frame=False, just_xy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to return the data for the entire sample, as opposed to one agent in the sample, you can use the `get_annotations_for_sample` method. This will return a list of records for each annotated agent in the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = helper.get_annotations_for_sample(sample_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are `get_future_for_sample` and `get_past_for_sample` methods that are analogous to the `get_future_for_agent` and `get_past_for_agent` methods.\n",
    "\n",
    "We also provide methods to compute the velocity, acceleration, and heading change rate of an agent at a given point in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2616850959705042"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We get a new instance and sample token because these methods require computing difference between records.\n",
    "instance_token_2, sample_token_2 = mini_train[5].split(\"_\")\n",
    "\n",
    "# Meters / second\n",
    "helper.get_velocity_for_agent(instance_token_2, sample_token_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2569005623454393"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Meters / second^2\n",
    "helper.get_acceleration_for_agent(instance_token_2, sample_token_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# radians / second\n",
    "helper.get_heading_change_rate_for_agent(instance_token_2, sample_token_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes to the Map API\n",
    "\n",
    "We've added a couple methods to the Map API to help query lane centerline information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: You are using an outdated map version! Please go to https://www.nuscenes.org/download to download the latest map!\n"
     ]
    }
   ],
   "source": [
    "from nuscenes.map_expansion.map_api import NuScenesMap\n",
    "onenorth = NuScenesMap(map_name='singapore-onenorth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the closest lane to a location, use the `get_closest_lane` method. To see the internal data representation of the lane, use the `get_lane_record` method. \n",
    "You can also explore the connectivity of the lanes, with the `get_outgoing_lanes` and `get_incoming_lane_methods`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5933500a-f0f2-4d69-9bbc-83b875e4a73e'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, yaw = 395, 1095, 0\n",
    "closest_lane = onenorth.get_closest_lane(x, y)\n",
    "closest_lane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start_pose': [421.2419602954602, 1087.9127960414617, 2.739593514975998],\n",
       "  'end_pose': [391.7142849867393, 1100.464077182952, 2.7365754617298705],\n",
       "  'shape': 'LSR',\n",
       "  'radius': 999.999,\n",
       "  'segment_length': [0.23651121617864976,\n",
       "   28.593481378991886,\n",
       "   3.254561444252876]}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lane_record = onenorth.get_lane(closest_lane)\n",
    "lane_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f24a067b-d650-47d0-8664-039d648d7c0d']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onenorth.get_incoming_lane_ids(closest_lane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0282d0e3-b6bf-4bcd-be24-35c9ce4c6591',\n",
       " '28d15254-0ef9-48c3-9e06-dc5a25b31127']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onenorth.get_outgoing_lane_ids(closest_lane)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help manipulate the lanes, we've added an `arcline_path_utils` module. For example, something you might want to do is discretize a lane into a sequence of poses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(421.2419602954602, 1087.9127960414617, 2.739593514975998),\n",
       " (420.34712994585345, 1088.2930152148274, 2.739830026428688),\n",
       " (419.45228865726136, 1088.6732086473173, 2.739830026428688),\n",
       " (418.5574473686693, 1089.0534020798073, 2.739830026428688),\n",
       " (417.66260608007724, 1089.433595512297, 2.739830026428688),\n",
       " (416.76776479148515, 1089.813788944787, 2.739830026428688),\n",
       " (415.87292350289306, 1090.1939823772768, 2.739830026428688),\n",
       " (414.97808221430097, 1090.5741758097668, 2.739830026428688),\n",
       " (414.0832409257089, 1090.9543692422567, 2.739830026428688),\n",
       " (413.1883996371168, 1091.3345626747464, 2.739830026428688),\n",
       " (412.29355834852475, 1091.7147561072363, 2.739830026428688),\n",
       " (411.39871705993266, 1092.0949495397263, 2.739830026428688),\n",
       " (410.5038757713406, 1092.4751429722162, 2.739830026428688),\n",
       " (409.6090344827485, 1092.8553364047061, 2.739830026428688),\n",
       " (408.7141931941564, 1093.2355298371958, 2.739830026428688),\n",
       " (407.81935190556436, 1093.6157232696858, 2.739830026428688),\n",
       " (406.92451061697227, 1093.9959167021757, 2.739830026428688),\n",
       " (406.0296693283802, 1094.3761101346656, 2.739830026428688),\n",
       " (405.1348280397881, 1094.7563035671556, 2.739830026428688),\n",
       " (404.239986751196, 1095.1364969996453, 2.739830026428688),\n",
       " (403.3451454626039, 1095.5166904321352, 2.739830026428688),\n",
       " (402.4503041740119, 1095.8968838646251, 2.739830026428688),\n",
       " (401.5554628854198, 1096.277077297115, 2.739830026428688),\n",
       " (400.6606215968277, 1096.657270729605, 2.739830026428688),\n",
       " (399.7657803082356, 1097.0374641620947, 2.739830026428688),\n",
       " (398.8709390196435, 1097.4176575945846, 2.739830026428688),\n",
       " (397.9760977310515, 1097.7978510270746, 2.739830026428688),\n",
       " (397.0812564424594, 1098.1780444595645, 2.739830026428688),\n",
       " (396.1864151538673, 1098.5582378920544, 2.739830026428688),\n",
       " (395.2915738652752, 1098.9384313245444, 2.739830026428688),\n",
       " (394.3967548911081, 1099.318677260896, 2.739492242286598),\n",
       " (393.5022271882191, 1099.69960782173, 2.738519982101022),\n",
       " (392.60807027168346, 1100.0814079160527, 2.737547721915446),\n",
       " (391.71428498673856, 1100.4640771829522, 2.7365754617298705)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nuscenes.map_expansion import arcline_path_utils\n",
    "poses = arcline_path_utils.discretize_lane(lane_record, resolution_meters=1)\n",
    "poses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a query pose, you can also find the closest pose on a lane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_pose_on_lane, distance_along_lane = arcline_path_utils.project_pose_to_lane((x, y, yaw), lane_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395 1095 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(396.25524909914367, 1098.5289922434013, 2.739830026428688)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x, y, yaw)\n",
    "closest_pose_on_lane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.5"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Meters\n",
    "distance_along_lane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the entire length of the lane, you can use the `length_of_lane` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.08455403942341"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arcline_path_utils.length_of_lane(lane_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also compute the curvature of a lane at a given length along the lane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0 Means it is a straight lane\n",
    "arcline_path_utils.get_curvature_at_distance_along_lane(distance_along_lane, lane_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Input Representation\n",
    "\n",
    "It is common in the prediction literature to represent the state of an agent as a tensor containing information about the semantic map (such as the drivable area and walkways) as well the past locations of surrounding agents.\n",
    "\n",
    "Each paper in the field chooses to represent the input in a slightly different way. For example, [CoverNet](https://arxiv.org/pdf/1911.10298.pdf) and [MTP](https://arxiv.org/pdf/1808.05819.pdf) choose to rasterize the map information and agent locations into a three channel RGB image. But [Rules of the Road](http://openaccess.thecvf.com/content_CVPR_2019/papers/Hong_Rules_of_the_Road_Predicting_Driving_Behavior_With_a_Convolutional_CVPR_2019_paper.pdf) decides to use a taller tensor with information represented in different channels .\n",
    "\n",
    "We provide a module called `input_representation` that is meant to make it easy for you to define your own input representation. In short, you need to define your own `StaticLayerRepresentation`, `AgentRepresentation`, and `Combinator`.\n",
    "\n",
    "The `StaticLayerRepresentation` controls how the static map information is represented. The `AgentRepresentation` controls how the locations of the agents in the scene are represented. The `Combinator` controls how these two sources of information are combined into a single tensor.\n",
    "\n",
    "For more information, consult `input_representation/interface.py`.\n",
    "\n",
    "To help get you started, we've provided implementations of input representation used in CoverNet and MTP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.predict.input_representation.static_layers import StaticLayerRasterizer\n",
    "from nuscenes.predict.input_representation.agents import AgentBoxesWithFadedHistory\n",
    "from nuscenes.predict.input_representation.interface import InputRepresentation\n",
    "from nuscenes.predict.input_representation.combinators import Rasterizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static_layers.py - Loading Map: singapore-onenorth\n"
     ]
    }
   ],
   "source": [
    "static_layer_rasterizer = StaticLayerRasterizer(helper)\n",
    "agent_rasterizer = AgentBoxesWithFadedHistory(helper)\n",
    "mtp_input_representation = InputRepresentation(static_layer_rasterizer, agent_rasterizer, Rasterizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns = [ann for ann in nuscenes.sample_annotation if ann['instance_token'] == instance_token_img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_token_img, sample_token_img = 'bc38961ca0ac4b14ab90e547ba79fbb6', '7626dde27d604ac28a0240bdd54eba7a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = mtp_input_representation.make_input_representation(instance_token_img, sample_token_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model Implementations\n",
    "\n",
    "We've provided PyTorch implementations for CoverNet and MTP. Below we show, how to make predictions on the previously created input representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.predict.models.backbone import ResNetBackbone\n",
    "from nuscenes.predict.models.mtp import MTP\n",
    "from nuscenes.predict.models.covernet import CoverNet\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = ResNetBackbone('resnet50')\n",
    "mtp = MTP(backbone, num_modes=2)\n",
    "\n",
    "# Note that the value of num_modes depends on the size of the lattice used for CoverNet\n",
    "covernet = CoverNet(backbone, num_modes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_state_vector = torch.Tensor([[helper.get_velocity_for_agent(instance_token_img, sample_token_img),\n",
    "                                    helper.get_acceleration_for_agent(instance_token_img, sample_token_img),\n",
    "                                    helper.get_heading_change_rate_for_agent(instance_token_img, sample_token_img)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor = torch.Tensor(img).permute(2, 0, 1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output has 50 entries.\n",
    "# The first 24 are x,y coordinates over the next 6 seconds at 2 Hz for the first mode.\n",
    "# The second 24 are the x,y coordinates for the second mode.\n",
    "# The last 2 are the logits of the mode probabilities\n",
    "mtp(image_tensor, agent_state_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoverNet outputs a probability distribution over the lattice.\n",
    "# These are the logits of the probabilities\n",
    "covernet(image_tensor, agent_state_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
